name: ingestao-continuada

on:
  workflow_dispatch:
  schedule:
    - cron: "*/15 * * * *"

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 50

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Instalar dependencias
        run: |
          python -m pip install --upgrade pip
          pip install requests python-slugify

      - name: Ingestão com imagem
        env:
          WC_BASE: ${{ secrets.WC_BASE }}
          WC_CK:   ${{ secrets.WC_CK }}
          WC_CS:   ${{ secrets.WC_CS }}
        run: |
          python - <<'PY'
          import os, re, time, json, random, html, hashlib
          import urllib.parse as up
          import requests

          BASE = os.environ["WC_BASE"].rstrip("/")
          CK   = os.environ["WC_CK"]
          CS   = os.environ["WC_CS"]

          S = requests.Session()
          S.headers.update({
            "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122 Safari/537.36",
            "Accept-Language": "pt-BR,pt;q=0.9,en;q=0.8",
          })

          ENGINES = [
            "https://www.bing.com/search?q=",
            "https://duckduckgo.com/html/?q=",
          ]

          DOMAINS = [
            "amazon.com.br","mercadolivre.com.br","shopee.com.br","shein.com",
            "aliexpress.com","magazineluiza.com.br","americanas.com.br",
            "submarino.com.br","kabum.com.br","casasbahia.com.br","pontofrio.com.br",
            "extra.com.br","carrefour.com.br","netshoes.com.br","centauro.com.br"
          ]

          BAD_PATH = (
            "login","cart","checkout","track","seller","support","help",
            "mailto:","account","orders","wishlist","entrar","minha-conta"
          )

          PRICE_RX = re.compile(
            r'(?:R\$\s*|R\$&nbsp;)?(\d{1,3}(?:[\.\s]\d{3})*(?:,\d{2})|\d+(?:,\d{2}))',
            re.I
          )

          def search_links(q: str):
            q_domains = " OR ".join([f"site:{d}" for d in DOMAINS])
            q_full = f"{q} {q_domains}"
            out = []
            for eng in ENGINES:
              url = eng + up.quote(q_full)
              try:
                r = S.get(url, timeout=25)
                r.raise_for_status()
              except Exception:
                continue
              for u in re.findall(r'https?://[^"\'\s<>]+', r.text):
                u = u.split("&")[0]
                if not u.startswith("http"):
                  continue
                if any(x in u for x in BAD_PATH):
                  continue
                out.append(u)
            seen, res = set(), []
            for u in out:
              if u not in seen:
                seen.add(u); res.append(u)
            return res[:120]

          def absolute_url(base, url):
            try:
              return up.urljoin(base, url)
            except Exception:
              return url

          def extract_meta(url: str):
            try:
              r = S.get(url, timeout=25)
              r.raise_for_status()
            except Exception:
              return None
            t = r.text

            def m(prop):
              a = re.search(rf'<meta[^>]+property=["\']{prop}["\'][^>]+content=["\']([^"\']+)["\']', t, re.I)
              if not a:
                a = re.search(rf'<meta[^>]+name=["\']{prop}["\'][^>]+content=["\']([^"\']+)["\']', t, re.I)
              return html.unescape(a.group(1).strip()) if a else ""

            title = m("og:title") or m("twitter:title")
            if not title:
              mt = re.search(r"<title>(.*?)</title>", t, re.I | re.S)
              title = mt.group(1).strip() if mt else ""

            img = m("og:image") or m("twitter:image")
            if not img:
              im = re.search(r'<img[^>]+src=["\']([^"\']+)["\']', t, re.I)
              if im:
                img = absolute_url(url, im.group(1))

            price = ""
            mp = PRICE_RX.search(t)
            if mp:
              price = mp.group(1).replace(".", "").replace(" ", "").replace(",", ".")

            if not title:
              return None
            return {"title": title[:180], "image": img, "price": price}

          def get_by_sku(sku: str):
            try:
              r = S.get(
                f"{BASE}/wp-json/wc/v3/products",
                params={"sku": sku, "per_page": 1, "consumer_key": CK, "consumer_secret": CS},
                timeout=20,
              )
              r.raise_for_status()
              arr = r.json()
              if arr:
                pid = arr[0]["id"]
                has_img = bool(arr[0].get("images"))
                return pid, has_img
            except Exception:
              pass
            return None, False

          def create_or_update(meta, url):
            sku = hashlib.md5(url.encode("utf-8")).hexdigest()[:12].upper()
            pid, has_img = get_by_sku(sku)
            data_img = [{"src": meta["image"]}] if meta["image"] else []
            api = f"{BASE}/wp-json/wc/v3/products"

            if pid and (not has_img) and data_img:
              try:
                r = S.put(f"{api}/{pid}",
                          params={"consumer_key": CK, "consumer_secret": CS},
                          json={"images": data_img}, timeout=25)
                r.raise_for_status()
                return "updated"
              except Exception:
                return "error"

            if pid:
              return "skipped"

            payload = {
              "name": meta["title"],
              "type": "simple",
              "status": "publish",
              "regular_price": meta["price"] or "0",
              "sku": sku,
              "external_url": url,
              "catalog_visibility": "visible",
              "short_description": f"Importado automaticamente. Fonte: {url}",
              "images": data_img,
              "categories": [],
              "tags": [],
            }
            try:
              r = S.post(api,
                         params={"consumer_key": CK, "consumer_secret": CS},
                         json=payload, timeout=25)
              r.raise_for_status()
              return "created"
            except Exception:
              return "error"

          QUERIES = [
            "smartphone 256gb promoção",
            "smartphone moto g54 256gb",
            "iphone 13 128gb preço",
            "smartwatch masculino aço 46mm",
            "headset gamer bluetooth",
            "ssd nvme 1tb",
            "tv 50 4k",
            "roteador wifi 6 ax3000",
            "caixa de som portátil",
            "fone bluetooth tws",
            "notebook i5 8gb ssd",
            "mouse gamer rgb",
            "fire tv stick 4k",
            "gabinete gamer rgb",
            "cadeira gamer ergonômica",
          ]

          random.shuffle(QUERIES)
          created = updated = skipped = 0
          max_per_run = 20

          for q in QUERIES:
            if created + updated >= max_per_run:
              break
            for u in search_links(q):
              if created + updated >= max_per_run:
                break
              meta = extract_meta(u)
              if not meta:
                continue
              res = create_or_update(meta, u)
              if res == "created":
                created += 1
              elif res == "updated":
                updated += 1
              elif res == "skipped":
                skipped += 1
              time.sleep(random.uniform(0.5, 1.3))

          print(json.dumps({"created": created, "updated": updated, "skipped": skipped}, ensure_ascii=False))
          PY
