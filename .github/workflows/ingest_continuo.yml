name: ingestao-continuada

on:
  workflow_dispatch:
  schedule:
    - cron: "*/15 * * * *"

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 50
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 lxml python-slugify dateparser tenacity

      - name: Ingestão global com imagem
        env:
          WC_BASE: ${{ secrets.WC_BASE }}
          WC_CK:   ${{ secrets.WC_CK }}
          WC_CS:   ${{ secrets.WC_CS }}
        run: |
          python - <<'PY'
          import os, re, time, json, random, html, hashlib, urllib.parse, requests, sys

          BASE = os.environ["WC_BASE"].rstrip("/")
          CK   = os.environ["WC_CK"]
          CS   = os.environ["WC_CS"]

          S = requests.Session()
          S.headers.update({
              "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122 Safari/537.36",
              "Accept-Language": "pt-BR,pt;q=0.9,en;q=0.8",
          })

          ENGINES = [
              "https://www.bing.com/search?q=",
              "https://duckduckgo.com/html/?q=",
          ]

          DOMAINS_PRIMARY = [
              "amazon.com.br","mercadolivre.com.br","shopee.com.br","magazineluiza.com.br",
              "americanas.com.br","submarino.com.br","kabum.com.br","casasbahia.com.br",
              "aliexpress.com","temu.com"
          ]
          DOMAINS_FALLBACK = []  # sem filtro de domínio no fallback

          BAD_PATH = (
              "login","cart","checkout","track","seller","support","help","mailto:",
              "account","orders","wishlist","entrar","minha-conta"
          )

          PRICE_RX = re.compile(
              r'(?:R\\$\\s*|\\$?\\s*)'
              r'([0-9]{1,3}(?:\\.[0-9]{3})*(?:,[0-9]{2})|[0-9]+(?:\\.[0-9]{2}))',
              re.I
          )

          def search_links(q: str, domains):
              out = []
              site_filter = " OR ".join(["site:" + d for d in domains]) if domains else ""
              q_full = f"{q} {site_filter}".strip()
              for eng in ENGINES:
                  url = eng + urllib.parse.quote(q_full)
                  try:
                      r = S.get(url, timeout=25); r.raise_for_status()
                  except Exception:
                      continue
                  for u in re.findall(r'https?://[^"\'\\s<>]+', r.text):
                      u = u.split("&")[0]
                      if not u.startswith("http"): continue
                      if any(x in u for x in BAD_PATH): continue
                      if (not domains) or any(d in u for d in domains):
                          out.append(u)
              seen, res = set(), []
              for u in out:
                  if u not in seen:
                      seen.add(u); res.append(u)
              return res[:150]

          def extract_meta(url: str):
              try:
                  r = S.get(url, timeout=30); r.raise_for_status()
              except Exception:
                  return None
              t = r.text

              def meta(prop):
                  m = re.search(rf'<meta[^>]+property=["\\']{prop}["\\'][^>]+content=["\\']([^"\\']+)["\\']', t, re.I)
                  if not m:
                      m = re.search(rf'<meta[^>]+name=["\\']{prop}["\\'][^>]+content=["\\']([^"\\']+)["\\']', t, re.I)
                  return html.unescape(m.group(1).strip()) if m else ""

              title = meta("og:title") or meta("twitter:title")
              if not title:
                  m = re.search(r"<title>(.*?)</title>", t, re.I|re.S)
                  title = m.group(1).strip() if m else ""

              img = meta("og:image") or meta("twitter:image")

              price = ""
              mp = PRICE_RX.search(t)
              if mp:
                  price = mp.group(1).replace(".", "").replace(",", ".")
              return {"title": title[:180] if title else "", "image": img, "price": price}

          def get_by_sku(sku: str):
              api = f"{BASE}/wp-json/wc/v3/products"
              try:
                  r = S.get(api, params={"sku": sku, "per_page": 1,
                                         "consumer_key": CK, "consumer_secret": CS}, timeout=25)
                  r.raise_for_status()
                  arr = r.json()
                  if arr:
                      item = arr[0]
                      pid = item["id"]
                      imgs = item.get("images") or []
                      return pid, bool(imgs)
              except Exception:
                  pass
              return None, False

          def create_or_update(meta, url):
              title = meta["title"]
              img   = meta["image"]
              price = meta["price"] or "0"
              sku   = hashlib.md5(url.encode("utf-8")).hexdigest()[:12].upper()

              api = f"{BASE}/wp-json/wc/v3/products"
              pid, has_img = get_by_sku(sku)

              if pid and (not has_img) and img:
                  try:
                      r = S.put(f"{api}/{pid}",
                               params={"consumer_key": CK, "consumer_secret": CS},
                               json={"images": [{"src": img}]}, timeout=30)
                      r.raise_for_status()
                      return "updated"
                  except Exception:
                      return "error"

              if pid:
                  return "skip"

              data = {
                  "name": title or "Produto afiliado",
                  "type": "simple",
                  "status": "publish",
                  "regular_price": price,
                  "sku": sku,
                  "external_url": url,
                  "catalog_visibility": "visible",
                  "short_description": f"Importado automaticamente. Fonte: {url}",
                  "images": ([{"src": img}] if img else []),
              }
              try:
                  r = S.post(api, params={"consumer_key": CK, "consumer_secret": CS},
                             json=data, timeout=30)
                  r.raise_for_status()
                  return "created"
              except Exception:
                  return "error"

          QUERIES = [
              "iphone 14 128gb preço","xiaomi redmi note 13 256gb",
              "galaxy s23 256gb oferta","notebook i5 16gb ssd 512",
              "macbook air m2 8gb 256","tv 55 4k hdr",
              "monitor 27 144hz","ssd nvme 1tb gen4",
              "placa de video rtx 4060","headset gamer bluetooth",
              "caixa de som portátil bt","fone bluetooth tws anc",
              "smartwatch amoled gps","roteador wi-fi 6 ax3000",
              "impressora tanque de tinta","câmera action 4k",
              "airfryer 5 litros","cafeteira espresso",
              "cadeira gamer ergonômica","kit ferramentas makita"
          ]

          def crawl(domains):
              created = updated = skipped = 0
              random.shuffle(QUERIES)
              max_per_run = 40
              for q in QUERIES:
                  if created + updated >= max_per_run: break
                  for u in search_links(q, domains):
                      if created + updated >= max_per_run: break
                      meta = extract_meta(u)
                      if not meta or not meta["title"]: continue
                      res = create_or_update(meta, u)
                      if res == "created": created += 1
                      elif res == "updated": updated += 1
                      elif res == "skip": skipped += 1
                      time.sleep(random.uniform(0.6, 1.2))
              return created, updated, skipped

          c1, u1, s1 = crawl(DOMAINS_PRIMARY)
          if c1 + u1 == 0:
              c2, u2, s2 = crawl(DOMAINS_FALLBACK)
          else:
              c2 = u2 = s2 = 0

          created = c1 + c2
          updated = u1 + u2
          skipped = s1 + s2

          print(json.dumps({"created": created, "updated": updated, "skipped": skipped}, ensure_ascii=False))

          # OBRIGATÓRIO trazer pelo menos 1
          if created + updated == 0:
              sys.exit(1)
          PY
